# TRANSMUT-Spark 

## Transformation Mutation for Apache Spark

TRANSMUT-Spark is a SBT plugin for mutation testing of Apache Spark programs in Scala. It applys transformation mutation to insert faults in the set of transformations called in a Spark program.

### Requirements
* SBT 1.3+
* Scala 2.12.x
* SemanticDB enabled (`semanticdbEnabled := true`)
* Spark programs using only the RDD (Core) API

### Mutation Operators

| **Operator** | **Description**                            |
|----------|----------------------------------------|
| UTS      | Unary Transformation Swap              |
| BTS      | Binary Transformation Swap             |
| UTR      | Unary Transformation Replacement       |
| BTR      | Binary Transformation Replacement      |
| UTD      | Unary Transformation Deletion          |
| MTR      | Mapping Transformation Replacement     |
| FTD      | Filter Transformation Deletion         |
| STR      | Set Transformation Replacement         |
| DTD      | Distinct Transformation Deletion       |
| DTI      | Distinct Transformation Insertion      |
| ATR      | Aggregation Transformation Replacement |
| JTR      | Join Transformation Replacement        |
| OTD      | Order Transformation Deletion          |

In the list of mutation operators in the TRANSMUT-Spark configurations (`mutation-operators`), use "ALL" as an alias to apply all mutation operators.

### Configurations

Configurations of TRANSMUT-Spark are set in the `transmut.conf` file in the root of the project. The file is in the HOCON format and all configuration options should be in the "transmut" namespace. For example:

```
transmut {
    sources: [ "WordCount.scala" ],
    programs: [ "wordCount" ]
}
```

#### Configuration Options:
* `sources`
	* **Description:** list of file names of the program source codes (Scala source codes with the programs to be mutated). Only the file name must be entered, not the full path.
	* **Mandatory:** Yes
	* **Example:** `sources: [ "WordCount.scala" ]`
* `programs`
	* **Description:** list of programs (methods) to be mutated. For TRANSMUT-Spark, a Spark program must be encapsulated in a method to be mutated. The programs in the list must exist in one of the sources. Only the methods in the list are mutated, other methods and statements of the sources remain unchanged.
	* **Mandatory:** Yes
	* **Example:** `programs: [ "wordCount" ]`
	* **Warning:** if the programs in the list are not identified in any source, the execution will fail.
* `mutation-operators`
	* **Description:** list of mutation operators to be applied in the mutation testing process. Only the mutation operators in the list are applied to generate mutants.
	* **Mandatory:** No
	* **Default:** `[ "ALL" ]`
	* **Example:** `mutation-operators: [ "DTI", "ATR", "JTR" ]`
* `equivalent-mutants`
	* **Description:** list of equivalent mutants IDs. This list must be updated after a first run of the tool and an analysis of the survived mutants if they are identified as equivalent. If the list of programs, original program codes and/or list of mutation operators are not changed, different executions of TRANSMUT-Spark will always generate the same mutants and with the same IDs. Mutants in this list are not executed in the tests and are marked as equivalent in the reports.
	* **Mandatory:** No
	* **Default:** ` [ ] `
	* **Example:** `equivalent-mutants: [ 3, 5, 6 ]`
	* **Warning:** if the list of programs, original program codes and/or list of mutation operators are changed after a run, the generated mutants and IDs may differ from those generated in the previous run in a new run.
* `test-only`
	* **Description:** list of test classes to be executed. If the list is empty, all tests of the project are executed. The full name of the test classes must be entered (package + class name).
	* **Mandatory:** No
	* **Default:** ` [ ] `
	* **Example:** `test-only: [ "examples.WordCountTest" ]`
	* **Warning:** run all tests of the project (in case of `test-only` is empty) can be a slow process on big projects or projects with many other tests that are not related with the programs to be mutated. In this case, specifying the test classes to be executed is recommended to speed up the process.
* `src-dir`
	* **Description:** The directory containing the sources of the programs to be mutated.
	* **Mandatory:** No
	* **Default:** `'src/main/scala/'` (scalaSource from SBT)
	* **Example:** `src-dir: 'other-src-folder/main/scala/'`
* `semanticdb-dir`
	* **Description:** The directory containing the semanticdb specifications generated by the SemanticDB compile plugin. TRANSMUT-Spark is dependent on SemanticDB compile plugin, so SemanticDB must be enabled in the project settings (`semanticdbEnabled := true` in the `build.sbt` file)
	* **Mandatory:** No
	* **Default:** `'target/scala-2.12/meta/'` (semanticdbTargetRoot from SBT)
	* **Example:** `semanticdb-dir: 'target/scala-2.12/other-meta/'`
	* **Warning:** if the SemanticDB compile plugin is not enabled in the project settings (`semanticdbEnabled := true` in the `build.sbt` file), the execution will fail.
* `transmut-dir`
	* **Description:** The directory containing the sources and reports generated by TRANSMUT-Spark.
	* **Mandatory:** No
	* **Default:** `'target/transmut/'`
	* **Example:** `transmut-dir: 'target/other-transmut-folder/'`



### Restrictions
* References (parameters, variables and values) must have unique names;
* Programs to be mutated must be encapsulated in methods;
* All RDDs must have their own reference (must be declared as a parameter, variable or value);
* Only one transformation must be called in a statement:
	* For example: `val rdd2 = rdd.filter( (a: String) => !a.isEmpty )`
* Anonymous (lambda) functions must have their input parameters explicitly typed:
	* Incorrect: `rdd.map( a => a * a )`
	* Correct: `rdd.map( (a: Int) => a * a )`

	
Example of a traditional Spark program that is not in the format supported by TRANSMUT-Spark:

```
package examples

import org.apache.spark._

object WordCount {

	def main(args: Array[String]): Unit = {
		val conf = new SparkConf()
    	conf.setAppName("Word-Count")
    	val sc = new SparkContext(conf)
		val textFile = sc.textFile("hdfs://...")
		val counts = textFile.flatMap(line => line.split(" "))
                 .map(word => (word, 1))
                 .reduceByKey(_ + _)
		counts.saveAsTextFile("hdfs://...")
	}
  
}
```

Example of the same program in a version supported by TRANSMUT-Spark:

```
package examples

import org.apache.spark._
import org.apache.spark.rdd.RDD

object WordCount {

	def wordCount(input: RDD[String]) = {
		val words = input.flatMap( (line: String) => line.split(" ") )
		val pairs = words.map( (word: String) => (word, 1) )
		val counts = pairs.reduceByKey( (a: Int, b: Int) => a + b )
		counts
	}
	
	def main(args: Array[String]): Unit = {
		val conf = new SparkConf()
    	conf.setAppName("Word-Count")
    	val sc = new SparkContext(conf)
		val textFile = sc.textFile("hdfs://...")
		val results = wordCount(textFile)
		results.saveAsTextFile("hdfs://...")
	}
  
}
```
	
Considering the example above, only the method `wordCount` should be included as a program to be mutated in the TRANSMUT-Spark configurations.

### Supported Transformations

| Transformation                 | Interface                                                                                                   |
|--------------------------------|-------------------------------------------------------------------------------------------------------------|
| map                            | `map[U](f: (T) ⇒ U): RDD[U]`                                                                                |
| flatMap                        | `flatMap[U](f: (T) ⇒ TraversableOnce[U]): RDD[U]`                                                           |
| filter                         | `filter(f: (T) ⇒ Boolean): RDD[T]`                                                                          |
| distinct                       | `distinct(): RDD[T]`                                                                                        |
| sortBy                         | `sortBy[K](f: (T) ⇒ K, ascending: Boolean = true): RDD[T]`                                                  |
| sortByKey                      | `sortByKey(ascending: Boolean = true): RDD[(K, V)]`                                                         |
| union                          | `union(other: RDD[T]): RDD[T]`                                                                              |
| intersection                   | `intersection(other: RDD[T]): RDD[T]`                                                                       |
| subtract                       | `subtract(other: RDD[T]): RDD[T]`                                                                           |
| join                           | `join[W](other: RDD[(K, W)]): RDD[(K, (V, W))]`                                                             |
| leftOuterJoin                  | `leftOuterJoin[W](other: RDD[(K, W)]): RDD[(K, (V, Option[W]))]`                                            |
| rightOuterJoin                 | `rightOuterJoin[W](other: RDD[(K, W)]): RDD[(K, (Option[V], W))]`                                           |
| fullOuterJoin                  | `fullOuterJoin[W](other: RDD[(K, W)]): RDD[(K, (Option[V], Option[W]))]`                                    |
| reduceByKey                    | `reduceByKey(func: (V, V) ⇒ V): RDD[(K, V)]`                                                                |
| combineByKey* (mergeCombiners) | `combineByKey[C](createCombiner: (V) ⇒ C, mergeValue: (C, V) ⇒ C, mergeCombiners: (C, C) ⇒ C): RDD[(K, C)]` |

<!--| aggregateByKey* (combOp) | `aggregateByKey[U](zeroValue: U)(seqOp: (U, V) ⇒ U, combOp: (U, U) ⇒ U): RDD[(K, U)]` |
-->
\* \- Only the parameter in parentheses is mutated, the rest remains the same.